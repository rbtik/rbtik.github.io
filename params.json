{"name":"rbtik.github.io","tagline":"","body":"#Welcome to my GitHub repository!\r\nThis website is where I store some of the work that I've been working on both in Python and in R. While some of these projects are assignments given for school, other projects were conducted outside of school as extracurricular points of interest. Other pieces of code may be simplistic but will serve to show what can be done with these tools with a few lines of simple and straight forward code (i.e. insightful and aesthetically pleasing data visualizations).\r\n\r\nPlease note: I will continuously be working on providing README files whenever possible. If there's none available at the moment for the project you're looking at, please let me know if you have any questions or would like something explained. Otherwise, I hope to have all projects explained as soon as possible.\r\n\r\n## Clustering Analysis\r\nIn this project, I was given 50 different data sets each containing close to 800 data points. Each data set represented a \"road\" or a \"train track\" or a \"field\" or anything that can come in between two antenna points. The data points found in each data set were the time in seconds it took to get from one antenna to the other antenna. The task was to use Clustering Techniques to identify which of the 25 data sets represented a road.\r\n\r\n[Road Clustering (Python)](http://nbviewer.ipython.org/github/rbtik/1415-Projects/blob/gh-pages/Clustering%20Example/Road%20Clustering.ipynb)\r\n\r\n[README](http://htmlpreview.github.io/?https://github.com/rbtik/1415-Projects/blob/gh-pages/Clustering%20Example/README.txt)\r\n\r\nUnfortunately, the data can not be made available at this point as it is proprietary.\r\n\r\n## Credit Scoring\r\nThis assignment uses the fairly infamous German Credit Scoring Data Set which classifies people described by a set of attributes as good or bad credit risks. The objective is to create a model than can predict if someone will be a good or bad credit risk.\r\n\r\n[German Credit Scoring Case (R)](http://nbviewer.ipython.org/github/rbtik/1415-Projects/blob/gh-pages/Credit%20Scoring/CreditScoring.ipynb)\r\n\r\n[Data](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29)\r\n\r\n\r\n## Data Visualization\r\nA classmate and I worked together on taking 1,000,000 gps traces from the recently released Uber data in San Francisco in order to create a data visualization. Throughout our work, we took the gps traces and mapped them against the districts in San Francisco and performed an analysis as to the activity that happened within and between the different districts. Our main purpose was to try to create a story as to how the traditional transportation infrastructure could be improved given users activities with Uber. \r\n\r\nAs there were two of us working on this project, we experimented with a collaboration tool called [Dataiku](http://www.dataiku.com/) which allowed us to write our own code which could then be easily linked and integrated within the tool. The following links are the codes that I contributed to the project (although there was a lot of collaboration so it is difficult to truly say we each worked on completely separate codes).\r\n\r\n[Extract Data from Uber data set (Python)] (http://htmlpreview.github.io/?https://github.com/rbtik/1415-Projects/blob/gh-pages/DataViz/extract_uber_data.html)\r\n\r\n[Number of Trips Matched Against Districts (Python)](http://htmlpreview.github.io/?https://github.com/rbtik/1415-Projects/blob/gh-pages/DataViz/Number%20of%20Trips%20Based%20on%20Districts.html)\r\n\r\n[Explore Trips (Python)](http://htmlpreview.github.io/?https://github.com/rbtik/1415-Projects/blob/gh-pages/DataViz/explore_trips.html)\r\n\r\n[Short Video Slice of Final Outcome](https://github.com/rbtik/1415-Projects/blob/gh-pages/DataViz/datavizvid.mp4)\r\n\r\n## Linear Regression Modeling\r\nIn this Linear Regression model, our group wanted to see if there was a relationship between the number of days needed to perform a clinical trial and various independent variables that surround data entry performance (i.e. if poor or missing fields and incorrect entries resulted in longer clinical trials). While not a standard linear regression study, the output of our attempt resulted in some interesting data visualizations during the data exploration piece. Although the resulting R-squared was humourously low and the model diagnostics were violated at every turn, this does set a good example of the method to implement linear regression and what needs to be taken into consideration when doing so.\r\n\r\n[Data Entry Performance on Clinical Trials (R)](http://htmlpreview.github.io/?https://github.com/rbtik/1415-Projects/blob/gh-pages/Data%20Entry/Clinical_Data_Entry.html)\r\n\r\nOnce again, the data is proprietary and is not available for sharing.\r\n\r\n## Loess Example\r\nIn this short assignment, we were tasked with understanding how the Loess method worked when graphing data. Data was scraped from the Wikipedia page of the historical winner times. The times for men and women were compared. Without the Loess method, the chart could be interpreted incorrectly in that female times could be faster than male times, but with this method, the true result becomes clearer.\r\n\r\n[Boston Marathon - Loess Example (R)](http://htmlpreview.github.io/?https://github.com/rbtik/1415-Projects/blob/gh-pages/Boston%20Marathon/Boston_Marathon_Loess.html) \r\n\r\n\r\n## Natural Language Processing\r\nAgain, this was a collaborative effort. Together with a classmate, we created a \"Crtics Consensus\" Tool. It is an automated sentence ranking and extraction tool made for the purpose of extracting the most important positive and the most important negative sentences within a corpus of documents. Specifically, the tool will be applied to a series of documents pertaining to movie reviews to get an overall understanding of the general thoughts and opinions of the reviewers. This one has a fairly extensive README file explaining the project and the process we took as well as the learnings that came from our experimentation with different NLP tools (like NLTK and senti-net). \r\n\r\n[Critics Consensus (Python)](http://htmlpreview.github.io/?https://github.com/rbtik/1415-Projects/blob/gh-pages/NLP%20Summarizing%20Tool/Compiled%20NLP%20Code.html)\r\n\r\n[README](https://github.com/rbtik/1415-Projects/blob/gh-pages/NLP%20Summarizing%20Tool/README.txt)\r\n\r\n[Frozen Reviews Data Set](https://github.com/rbtik/1415-Projects/tree/gh-pages/NLP%20Summarizing%20Tool/Frozen%20Reviews)\r\n\r\n\r\n###I hope you enjoyed looking through these projects. I am planning on keeping this page updated with more explanations and of course new projects. Please feel free to contact me if you have any questions or comments. Thank you!","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}